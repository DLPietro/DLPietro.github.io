---
layout: post
title: "ðŸ™ï¸ Louisville Payroll Analytics â€“ Part 1"
date: 2025-11-10 19:15:00 +0200
categories: public-sector
tags: [sql, postgresql, dataengineering, tableau, payroll, analytics]
---

# From Payroll Chaos to Structured Analytics

I was tired of looking at public data as a huge CSV and _not really understanding anything_: I wanted to go beyond â€œopen data as a fileâ€ and build an actual **analytics pipeline** creating SQL queries, views, and a live dashboard.

It turned into a month-long journey full of permission errors, failed imports, wrong joins, and a lot of satisfaction when things finally clicked.

> **Turn the raw public sector salaries of a city (starting with Louisville) into a clean PostgreSQL schema + SQL views + a Tableau Public dashboard** that tells us a story about salaries, overtime and equity.

---

# âš½ Main Goals

> Set up a working PostgreSQL environment (with DBeaver as SQL client)  
> Import the real **Louisville Metro payroll CSV** into a proper table  
> Design a SQL schema and views to support analytics (salary distribution, overtime, inequality)  
> Build a Tableau dashboard with 3â€“4 core views for business-style storytelling  
> Document everything in a GitHub repo so the work is **reproducible**, not a oneâ€‘off experiment  

---

# ðŸ“‹ Step by Step

ðŸ“ **Step 1 â€“ Losing time on PostgreSQL permissions**  
Creating a simple table should be easy, right? It wasnâ€™t. I hit `permission denied for schema public` more times than I can count. Fixing roles, grants, and reconnecting from DBeaver took longer than expected, but it forced me to really understand how PostgreSQL handles users and schemas.

ðŸ“ **Step 2 â€“ Designing the `salary_data` table and loading the CSV**  
Once permissions were fixed, I created a clean table for the dataset and used DBeaverâ€™s import wizard. First attempt failed because of NOT NULL constraints and column mismatches. After adjusting the mapping (especially `cal_year`, `ytd_total`, and overtime columns), the dataset finally landed in PostgreSQL.

ðŸ“ **Step 4 â€“ Connecting Tableau Public and shaping the dashboard**  
The last mile was visual. I connected Tableau directly to the views, built a yearly trend view (headcount + overtime %), a salaryâ€‘byâ€‘department chart, an overtimeâ€‘byâ€‘department chart, and a payâ€‘rangeâ€‘byâ€‘jobâ€‘title view for equity analysis.

# âš¡ PostgreSQL & DBeaver: The Hard Part No One Shows

The biggest hidden time sink in this project was not SQL logic, but **environment setup**:

- Wrong role: my user didnâ€™t have privileges on schema `public`  
- DBeaver kept failing silently on `CREATE TABLE` until I checked the server logs  
- CSV import initially broke because the header mapping didnâ€™t match the table schema and the first line was being treated as data, not as header  

It sounds trivial on paper, but when you hit these problems at midnight after a long day, itâ€™s easy to get discouraged.

Once I slowed down and treated the database like a real production system â€” checking roles, testing small queries, validating row counts â€” things became stable. At that point I could finally focus on **analysis instead of firefighting**.

---

# ðŸ“Š Structuring the Louisville Dataset

The original CSV contains thousands of rows with fields like year, employee name, department, job title, annual and yearâ€‘toâ€‘date pay, plus overtime and allowances.

I modeled it into a single fact table:

```sql
CREATE TABLE salary_data (
id SERIAL PRIMARY KEY,
cal_year INT NOT NULL,
employee_name VARCHAR(255) NOT NULL,
department VARCHAR(100) NOT NULL,
job_title VARCHAR(150) NOT NULL,
annual_rate NUMERIC(10, 2),
regular_rate NUMERIC(10, 2),

```

---

# ðŸ† Final Thoughts: The Other Side of the Dashboard

After being inside this project, I can say one thing: building dashboards and writing SQL queries was not easy, but the real challenge is understanding **what kind of world that data comes from**.

This dataset, as mentioned before, comes from the **U.S. banking market**, and you can feel it; why? Americans have a completely different relationship with money: **credit is not a danger**, itâ€™s a tool: they borrow to invest and build; the debt, for them, is justified as part of growth.

In Europe (especially in Southern Europe) itâ€™s the opposite story: **we save before spending**, we find it scary and dangerous.  

So while analyzing this dataset, I genuinely found myself thinking:

> â€œIf these were European customers, half these behaviors would never appear.â€

Thatâ€™s the interesting part: the same SQL queries, the same segmentation logic, but a completely different **financial psychology** behind the numbers, a **context** that has to be understood.  

That's exactly the reason why I'm passionate about this work, because I start with columns, rows, queries and dashboard, and then I end up questioning **why people are doing this or that!!**

ðŸ’¬ *Next step?* Maybe Iâ€™ll rebuild this entire analysis using **European-style banking data**, to see what happens when â€œcreditâ€ becomes the exception, not the rule.

---

# âš¡ Credits

[![GitHub Profile](https://img.shields.io/badge/GitHub-DLPietro-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/DLPietro)
[![Email](https://img.shields.io/badge/Email-dileopie-d14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:dileopie@gmail.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Pietro-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/pietrodileo)

> _Â© 2025 Pietro Di Leo. From Operations to Data, one Commit at a Time._